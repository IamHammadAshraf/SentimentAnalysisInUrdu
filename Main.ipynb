{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = url = \"https://github.com/MuhammadYaseenKhan/Urdu-Sentiment-Corpus/blob/master/urdu-sentiment-corpus-v1.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "p3sDmeg_wBBG",
        "outputId": "9e2d32fa-8cd6-4afd-c8fa-431069fe678b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Tweet Class\n",
              "0    میں نے ایٹم بم بنایا ھے ۔۔۔۔او بھائی ایٹم بمب ...     P\n",
              "1    چندے سے انقلاب اور عمران خان وزیر اعظم نہیں بن...     N\n",
              "2                             ٹویٹر کا خیال کیسے آیا ؟     O\n",
              "3    سرچ انجن گوگل کے نائب صدر نے فضا میں ، 130,000...     P\n",
              "4      ابھی تک اسکی لہریں کبھی کبھی آ جاتی ہیں یار :أْ     P\n",
              "..                                                 ...   ...\n",
              "995     اُس آدمی نے اِس سالار کو کافی معقول ٹپ دی ہے ۔     P\n",
              "996  چچا غالب کی روح سے معذرت کے ساتھہم نے مانا کہ ...     P\n",
              "997  واہ جناب واہ! اچھی رہی۔ جناب خود کو فرشتہ سمجو...     P\n",
              "998  اسلام آباد :پی اے ٹی کا دھرنا ختم، صفائی کے کا...     P\n",
              "999  دنیا نے کس کا راہ وفا میں دیا ہے ساتھتم بھی چل...     P\n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4025d036-e2d6-4c34-8a20-5a5794938aae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>میں نے ایٹم بم بنایا ھے ۔۔۔۔او بھائی ایٹم بمب ...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>چندے سے انقلاب اور عمران خان وزیر اعظم نہیں بن...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ٹویٹر کا خیال کیسے آیا ؟</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>سرچ انجن گوگل کے نائب صدر نے فضا میں ، 130,000...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ابھی تک اسکی لہریں کبھی کبھی آ جاتی ہیں یار :أْ</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>اُس آدمی نے اِس سالار کو کافی معقول ٹپ دی ہے ۔</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>چچا غالب کی روح سے معذرت کے ساتھہم نے مانا کہ ...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>واہ جناب واہ! اچھی رہی۔ جناب خود کو فرشتہ سمجو...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>اسلام آباد :پی اے ٹی کا دھرنا ختم، صفائی کے کا...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>دنیا نے کس کا راہ وفا میں دیا ہے ساتھتم بھی چل...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4025d036-e2d6-4c34-8a20-5a5794938aae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4025d036-e2d6-4c34-8a20-5a5794938aae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4025d036-e2d6-4c34-8a20-5a5794938aae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23c7693d-5cb5-456a-8164-3948c033ac94\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23c7693d-5cb5-456a-8164-3948c033ac94')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23c7693d-5cb5-456a-8164-3948c033ac94 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 999,\n        \"samples\": [\n          \" \\u0627\\u06cc\\u06a9 \\u06af\\u06be\\u0646\\u0679\\u06d2 \\u0628\\u0639\\u062f \\u06a9\\u06cc\\u0627 \\u06a9\\u0631\\u0648 \\u06af\\u06cc\\u061f\",\n          \"\\u0627\\u0644\\u0644\\u06c1 \\u0631\\u0628 \\u0627\\u0644\\u0639\\u0632\\u062a \\u06a9\\u0627 \\u0630\\u06a9\\u0631 \\u06a9\\u06cc\\u062c\\u06cc\\u06d2 \\u0627\\u0648\\u0631 \\u0633\\u06a9\\u0648\\u0646\\u0650 \\u0642\\u0644\\u0628 \\u062d\\u0627\\u0635\\u0644 \\u06a9\\u06cc\\u062c\\u06cc\\u06d2 \\u0633\\u0628\\u062d\\u0627\\u0646 \\u0627\\u0644\\u0644\\u0647 \\u060c\\u0627\\u0644\\u062d\\u0645\\u062f \\u0644\\u0644\\u0647 \\u060c\\u0627\\u0644\\u0644\\u0647 \\u0623\\u0643\\u0628\\u0631\\u060c\\u0644\\u0627\\u0627\\u0644\\u0647 \\u0627\\u0644\\u0627 \\u0627\\u0644\\u0644\\u0647 \\u060c\\u0644\\u0627\\u062d\\u0648\\u0644 \\u0648\\u0644\\u0627\\u0642\\u0648\\u0629 \\u0627\\u0644\\u0627 \\u0628\\u0627\\u0644\\u0644\\u0647\",\n          \"\\u067e\\u0627\\u06a9\\u0633\\u062a\\u0627\\u0646 \\u06a9\\u06cc \\u0642\\u0648\\u0645\\u06cc \\u06a9\\u0631\\u06a9\\u0679 \\u0679\\u06cc\\u0645 \\u06a9\\u06d2 \\u0633\\u0627\\u0628\\u0642 \\u06a9\\u067e\\u062a\\u0627\\u0646 \\u0634\\u0639\\u06cc\\u0628 \\u0645\\u0644\\u06a9 \\u06a9\\u0627 \\u0628\\u0627\\u0648\\u0674\\u0644\\u0646\\u06af \\u0627\\u06cc\\u06a9\\u0634\\u0646 \\u0628\\u06be\\u06cc \\u0645\\u0634\\u06a9\\u0648\\u06a9 \\u0642\\u0631\\u0627\\u0631 \\u062f\\u06d2 \\u062f\\u06cc\\u0627 \\u06af\\u06cc\\u0627\\u060c \\u0631\\u0648\\u0632\\u0646\\u0627\\u0645\\u06c1 \\u0627\\u064f\\u0631\\u062f\\u0648 \\u067e\\u0648\\u0627\\u0626\\u0646\\u0679\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"P\",\n          \"N\",\n          \"O\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('Class').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "OjUA5p7hwIZ_",
        "outputId": "8575dd7b-6f08-4512-ddeb-031b26881bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1**"
      ],
      "metadata": {
        "id": "2yxL57qH3e9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM**"
      ],
      "metadata": {
        "id": "JsFl3AKbyqO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "def train_and_evaluate_lstm_models(X_train, X_test, y_train, y_test, max_combinations=4):\n",
        "    # Load and preprocess the dataset\n",
        "    df = pd.read_csv(dataset_path, sep='\\t').dropna()\n",
        "    class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "    df['Class'] = df['Class'].map(class_mapping)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['Tweet'], df['Class'], test_size=0.25, random_state=42)\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    max_sequence_length = 100\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "    # Define hyperparameters to try\n",
        "    num_layers_values = [2, 3]\n",
        "    dropout_rate_values = [0.3, 0.7]\n",
        "    lstm_units_values = [64, 128]\n",
        "    embedding_dim_values = [25, 50]\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    # Iterate over hyperparameters\n",
        "    for num_layers in num_layers_values:\n",
        "        for dropout_rate in dropout_rate_values:\n",
        "            for lstm_units in lstm_units_values:\n",
        "                for embedding_dim in embedding_dim_values:\n",
        "                    print(f\"Training model with {num_layers} layers, dropout rate {dropout_rate}, LSTM units {lstm_units}, and embedding dimension {embedding_dim}...\")\n",
        "                    # Build the LSTM model\n",
        "                    model = Sequential([\n",
        "                        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "                        *[LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True) for _ in range(num_layers - 1)],\n",
        "                        LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
        "                        Dense(units=3, activation='softmax')\n",
        "                    ])\n",
        "                    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "                    history = model.fit(X_train_padded, y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.2)\n",
        "                    y_pred_prob = model.predict(X_test_padded)\n",
        "                    y_pred = y_pred_prob.argmax(axis=1)\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "                    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "                    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "                    # Store results\n",
        "                    results[(num_layers, dropout_rate, lstm_units, embedding_dim)] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "                    # Check if maximum combinations reached\n",
        "                    if len(results) >= max_combinations:\n",
        "                        return results\n",
        "    # Return results\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "iYGmH3sbtUpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN**"
      ],
      "metadata": {
        "id": "0r7QptDByun2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "def train_and_evaluate_rnn_models(dataset_path, max_combinations=4):\n",
        "    # Load and preprocess the dataset\n",
        "    df = pd.read_csv(dataset_path, sep='\\t').dropna()\n",
        "    class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "    df['Class'] = df['Class'].map(class_mapping)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['Tweet'], df['Class'], test_size=0.25, random_state=42)\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    max_sequence_length = 100\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "    # Define hyperparameters to try\n",
        "    num_layers_values = [2, 3]\n",
        "    dropout_rate_values = [0.3, 0.7]\n",
        "    rnn_units_values = [64, 128]\n",
        "    embedding_dim_values = [25, 50]\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    # Iterate over hyperparameters\n",
        "    for num_layers in num_layers_values:\n",
        "        for dropout_rate in dropout_rate_values:\n",
        "            for rnn_units in rnn_units_values:\n",
        "                for embedding_dim in embedding_dim_values:\n",
        "                    print(f\"Training model with {num_layers} layers, dropout rate {dropout_rate}, RNN units {rnn_units}, and embedding dimension {embedding_dim}...\")\n",
        "                    # Build the RNN model\n",
        "                    model = Sequential([\n",
        "                        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "                        *[SimpleRNN(units=rnn_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True) for _ in range(num_layers - 1)],\n",
        "                        SimpleRNN(units=rnn_units, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
        "                        Dense(units=3, activation='softmax')\n",
        "                    ])\n",
        "                    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "                    history = model.fit(X_train_padded, y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.2)\n",
        "                    y_pred_prob = model.predict(X_test_padded)\n",
        "                    y_pred = y_pred_prob.argmax(axis=1)\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "                    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "                    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "                    # Store results\n",
        "                    results[(num_layers, dropout_rate, rnn_units, embedding_dim)] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "                    # Check if maximum combinations reached\n",
        "                    if len(results) >= max_combinations:\n",
        "                        return results\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "bIecAi35t6pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRU**"
      ],
      "metadata": {
        "id": "KjH6cnG7zjGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "\n",
        "def train_and_evaluate_gru_models(X_train, X_test, y_train, y_test, max_combinations=4):\n",
        "    # Load and preprocess the dataset\n",
        "    df = pd.read_csv(dataset_path, sep='\\t').dropna()\n",
        "    class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "    df['Class'] = df['Class'].map(class_mapping)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['Tweet'], df['Class'], test_size=0.25, random_state=42)\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    max_sequence_length = 100\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "    # Define hyperparameters to try\n",
        "    num_layers_values = [2, 3]\n",
        "    dropout_rate_values = [0.3, 0.7]\n",
        "    gru_units_values = [64, 128]\n",
        "    embedding_dim_values = [25, 50]\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    # Iterate over hyperparameters\n",
        "    for num_layers in num_layers_values:\n",
        "        for dropout_rate in dropout_rate_values:\n",
        "            for gru_units in gru_units_values:\n",
        "                for embedding_dim in embedding_dim_values:\n",
        "                    print(f\"Training model with {num_layers} layers, dropout rate {dropout_rate}, GRU units {gru_units}, and embedding dimension {embedding_dim}...\")\n",
        "                    # Build the GRU model\n",
        "                    model = Sequential()\n",
        "                    model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "                    for _ in range(num_layers - 1):\n",
        "                        model.add(GRU(units=gru_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
        "                    model.add(GRU(units=gru_units, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
        "                    model.add(Dense(units=3, activation='softmax'))  # 3 output units for 3 classes\n",
        "\n",
        "                    # Compile the model\n",
        "                    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "                    # Train the model with early stopping\n",
        "                    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "                    history = model.fit(X_train_padded, y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "                    # Evaluate the model\n",
        "                    y_pred_prob = model.predict(X_test_padded)\n",
        "                    y_pred = y_pred_prob.argmax(axis=1)\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "                    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "                    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "                    # Store results\n",
        "                    results[(num_layers, dropout_rate, gru_units, embedding_dim)] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "                    # Check if maximum combinations reached\n",
        "                    if len(results) >= max_combinations:\n",
        "                        return results\n",
        "\n",
        "    # Return results\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "YX9-ipsst7GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BI-LSTM**"
      ],
      "metadata": {
        "id": "jBpPgYWozbTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "def train_and_evaluate_bilstm_models(X_train, X_test, y_train, y_test, max_combinations=4):\n",
        "    # Load and preprocess the dataset\n",
        "    df = pd.read_csv(dataset_path, sep='\\t').dropna()\n",
        "    class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "    df['Class'] = df['Class'].map(class_mapping)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['Tweet'], df['Class'], test_size=0.25, random_state=42)\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    max_sequence_length = 100\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "    # Define hyperparameters to try\n",
        "    num_layers_values = [2, 3]\n",
        "    dropout_rate_values = [0.3, 0.7]\n",
        "    lstm_units_values = [64, 128]  # Reduce LSTM units\n",
        "    embedding_dim_values = [25, 50]  # Reduce embedding dimension\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    # Iterate over hyperparameters\n",
        "    for num_layers in num_layers_values:\n",
        "        for dropout_rate in dropout_rate_values:\n",
        "            for lstm_units in lstm_units_values:\n",
        "                for embedding_dim in embedding_dim_values:\n",
        "                    print(f\"Training model with {num_layers} layers, dropout rate {dropout_rate}, LSTM units {lstm_units}, and embedding dimension {embedding_dim}...\")\n",
        "                    # Build the BiLSTM model\n",
        "                    model = Sequential()\n",
        "                    model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "                    for _ in range(num_layers - 1):\n",
        "                        model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
        "                    model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)))\n",
        "                    model.add(Dense(units=3, activation='softmax'))  # 3 output units for 3 classes\n",
        "\n",
        "                    # Compile the model\n",
        "                    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "                    # Train the model with early stopping\n",
        "                    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "                    history = model.fit(X_train_padded, y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "                    # Evaluate the model\n",
        "                    y_pred_prob = model.predict(X_test_padded)\n",
        "                    y_pred = y_pred_prob.argmax(axis=1)\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "                    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "                    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "                    # Store results\n",
        "                    results[(num_layers, dropout_rate, lstm_units, embedding_dim)] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "                    # Check if maximum combinations reached\n",
        "                    if len(results) >= max_combinations:\n",
        "                        return results\n",
        "\n",
        "    # Return results\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "vq5H8Ysfwj-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    dataset_path = \"urdu-sentiment-corpus-v1.tsv\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['Tweet'], df['Class'], test_size=0.25, random_state=42)\n",
        "    try:\n",
        "        results_rnn = train_and_evaluate_rnn_models(dataset_path)\n",
        "        assert results_rnn is not None\n",
        "        print(\"\\nRNN Results:\")\n",
        "        print_results(results_rnn)\n",
        "    except AssertionError:\n",
        "        print(\"RNN model failed. Moving to the next model.\")\n",
        "\n",
        "    try:\n",
        "        results_bilstm = train_and_evaluate_bilstm_models(X_train, X_test, y_train, y_test)\n",
        "        assert results_bilstm is not None\n",
        "        print(\"\\nBiLSTM Results:\")\n",
        "        print_results(results_bilstm)\n",
        "    except AssertionError:\n",
        "        print(\"BiLSTM model failed. Moving to the next model.\")\n",
        "\n",
        "    try:\n",
        "        results_gru = train_and_evaluate_gru_models(dataset_path)\n",
        "        assert results_gru is not None\n",
        "        print(\"\\nGRU Results:\")\n",
        "        print_results(results_gru)\n",
        "    except AssertionError:\n",
        "        print(\"GRU model failed.\")\n",
        "\n",
        "    try:\n",
        "        results_lstm = train_and_evaluate_lstm_models(X_train, X_test, y_train, y_test)\n",
        "        assert results_lstm is not None\n",
        "        print(\"\\nLSTM Results:\")\n",
        "        print_results(results_lstm)\n",
        "    except AssertionError:\n",
        "        print(\"LSTM model failed.\")\n",
        "\n",
        "def print_results(results):\n",
        "    print(\"Number of Layers\\tDropout Rate\\tUnits\\tEmbedding Dimension\\tAccuracy\\tPrecision\\tRecall\\tF1-Score\")\n",
        "    for key, value in results.items():\n",
        "        print(f\"{key[0]}\\t\\t\\t{key[1]}\\t\\t{key[2]}\\t{key[3]}\\t\\t\\t{value['Accuracy']:.4f}\\t\\t{value['Precision']:.4f}\\t\\t{value['Recall']:.4f}\\t\\t{value['F1-Score']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Ma-XQ5kZxSpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2**"
      ],
      "metadata": {
        "id": "0QFKQKrC3ZmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from gensim.models import FastText\n",
        "\n",
        "# Load the dataset\n",
        "url = \"urdu-sentiment-corpus-v1.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# Replace class labels with numerical values\n",
        "class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "df['Class'] = df['Class'].map(class_mapping)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Split data into input (X) and target (y) variables\n",
        "X = df['Tweet']\n",
        "y = df['Class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text data to sequences of integers\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = 100  # Adjust this value according to your data\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "# Train FastText embeddings\n",
        "print(\"Training FastText embeddings...\")\n",
        "fasttext_model = FastText(sentences=X, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, fasttext_model.wv.vector_size))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in fasttext_model.wv:\n",
        "        embedding_matrix[i] = fasttext_model.wv[word]\n",
        "\n",
        "# Define hyperparameters\n",
        "num_layers = 2\n",
        "dropout_rate = 0.3\n",
        "lstm_units = 64\n",
        "\n",
        "# Build the BiLSTM model with FastText embeddings\n",
        "print(\"Building BiLSTM model with FastText embeddings...\")\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=fasttext_model.wv.vector_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=True))\n",
        "for _ in range(num_layers - 1):\n",
        "    model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)))\n",
        "model.add(Dense(units=3, activation='softmax'))  # 3 output units for 3 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "print(\"Training BiLSTM model with FastText embeddings...\")\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "history = model.fit(X_train_padded, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating BiLSTM model with FastText embeddings...\")\n",
        "y_pred_prob = model.predict(X_test_padded)\n",
        "y_pred = y_pred_prob.argmax(axis=1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Store results\n",
        "rf = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "# Print results\n",
        "print(\"Results with BiLSTM and FastText embeddings:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky-a7kDG06bO",
        "outputId": "5a5efeb6-170e-4b5c-8c9b-254b1ddc5a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FastText embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building BiLSTM model with FastText embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BiLSTM model with FastText embeddings...\n",
            "Epoch 1/20\n",
            "19/19 [==============================] - 40s 1s/step - loss: 0.9127 - accuracy: 0.4825 - val_loss: 0.7355 - val_accuracy: 0.5200\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - 26s 1s/step - loss: 0.7925 - accuracy: 0.4925 - val_loss: 0.7554 - val_accuracy: 0.4733\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - 33s 2s/step - loss: 0.7258 - accuracy: 0.5776 - val_loss: 0.7434 - val_accuracy: 0.5600\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - 28s 1s/step - loss: 0.3151 - accuracy: 0.9065 - val_loss: 1.1514 - val_accuracy: 0.5267\n",
            "Evaluating BiLSTM model with FastText embeddings...\n",
            "8/8 [==============================] - 2s 111ms/step\n",
            "Results with BiLSTM and FastText embeddings:\n",
            "Accuracy: 0.64\n",
            "Precision: 0.625723602484472\n",
            "Recall: 0.64\n",
            "F1-Score: 0.6310632478632479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from gensim.models import FastText\n",
        "\n",
        "def train_evaluate_fasttext_bilstm(url, max_sequence_length=100, num_layers=2, dropout_rate=0.3, lstm_units=64, epochs=20, batch_size=32, validation_split=0.2, patience=3):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "    # Replace class labels with numerical values\n",
        "    class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "    df['Class'] = df['Class'].map(class_mapping)\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Split data into input (X) and target (y) variables\n",
        "    X = df['Tweet']\n",
        "    y = df['Class']\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    # Convert text data to sequences of integers\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "    # Train FastText embeddings\n",
        "    print(\"Training FastText embeddings...\")\n",
        "    fasttext_model = FastText(sentences=X, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "    # Create embedding matrix\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, fasttext_model.wv.vector_size))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in fasttext_model.wv:\n",
        "            embedding_matrix[i] = fasttext_model.wv[word]\n",
        "\n",
        "    # Build the BiLSTM model with FastText embeddings\n",
        "    print(\"Building BiLSTM model with FastText embeddings...\")\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=fasttext_model.wv.vector_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=True))\n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)))\n",
        "    model.add(Dense(units=3, activation='softmax'))  # 3 output units for 3 classes\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    print(\"Training BiLSTM model with FastText embeddings...\")\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
        "    history = model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=validation_split, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Evaluating BiLSTM model with FastText embeddings...\")\n",
        "    y_pred_prob = model.predict(X_test_padded)\n",
        "    y_pred = y_pred_prob.argmax(axis=1)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Store results\n",
        "    results = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "TmFjwsxd47ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_word2vec_bilstm(url, max_sequence_length=100, num_layers=2, dropout_rate=0.3, lstm_units=64, epochs=20, batch_size=32, validation_split=0.2, patience=3):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "    from gensim.models import Word2Vec\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "    # Replace class labels with numerical values\n",
        "    class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "    df['Class'] = df['Class'].map(class_mapping)\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Split data into input (X) and target (y) variables\n",
        "    X = df['Tweet']\n",
        "    y = df['Class']\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    # Convert text data to sequences of integers\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "    # Train Word2Vec embeddings\n",
        "    print(\"Training Word2Vec embeddings...\")\n",
        "    word2vec_model = Word2Vec(sentences=X, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "    # Create embedding matrix\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, word2vec_model.wv.vector_size))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "    # Build the BiLSTM model with Word2Vec embeddings\n",
        "    print(\"Building BiLSTM model with Word2Vec embeddings...\")\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=word2vec_model.wv.vector_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=True))\n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)))\n",
        "    model.add(Dense(units=3, activation='softmax'))  # 3 output units for 3 classes\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    print(\"Training BiLSTM model with Word2Vec embeddings...\")\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
        "    history = model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=validation_split, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Evaluating BiLSTM model with Word2Vec embeddings...\")\n",
        "    y_pred_prob = model.predict(X_test_padded)\n",
        "    y_pred = y_pred_prob.argmax(axis=1)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Store results\n",
        "    results = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "69wLZQFS5VZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_bilstm_without_pretrained_embeddings(url, max_sequence_length=100, num_layers=2, dropout_rate=0.3, lstm_units=64, epochs=20, batch_size=32, validation_split=0.2, patience=3):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "    # Replace class labels with numerical values\n",
        "    class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "    df['Class'] = df['Class'].map(class_mapping)\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Split data into input (X) and target (y) variables\n",
        "    X = df['Tweet']\n",
        "    y = df['Class']\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    # Convert text data to sequences of integers\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "    # Build the BiLSTM model without pre-trained embeddings\n",
        "    print(\"Building BiLSTM model without pre-trained embeddings...\")\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))  # Add this line to keep consistency in the indexing\n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)))\n",
        "    model.add(Dense(units=3, activation='softmax'))  # 3 output units for 3 classes\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    print(\"Training BiLSTM model without pre-trained embeddings...\")\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
        "    history = model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=validation_split, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Evaluating BiLSTM model without pre-trained embeddings...\")\n",
        "    y_pred_prob = model.predict(X_test_padded)\n",
        "    y_pred = y_pred_prob.argmax(axis=1)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Store results\n",
        "    results = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "xNee9Sko5-9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_bilstm_with_glove_embeddings(url, max_sequence_length=100, num_layers=2, dropout_rate=0.3, lstm_units=64, epochs=20, batch_size=32, validation_split=0.2, patience=3):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "    import gensim.downloader as api\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "    # Replace class labels with numerical values\n",
        "    class_mapping = {'P': 0, 'N': 1, 'O': 2}\n",
        "    df['Class'] = df['Class'].map(class_mapping)\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Split data into input (X) and target (y) variables\n",
        "    X = df['Tweet']\n",
        "    y = df['Class']\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    # Convert text data to sequences of integers\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "    # Download and load pre-trained GloVe embeddings\n",
        "    glove_model = api.load('glove-wiki-gigaword-100')\n",
        "\n",
        "    # Create embedding matrix\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, glove_model.vector_size))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in glove_model:\n",
        "            embedding_matrix[i] = glove_model[word]\n",
        "\n",
        "    # Build the BiLSTM model with GloVe embeddings\n",
        "    print(\"Building BiLSTM model with GloVe embeddings...\")\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=glove_model.vector_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=True))\n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)))\n",
        "    model.add(Dense(units=3, activation='softmax'))  # 3 output units for 3 classes\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    print(\"Training BiLSTM model with GloVe embeddings...\")\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
        "    history = model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=validation_split, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Evaluating BiLSTM model with GloVe embeddings...\")\n",
        "    y_pred_prob = model.predict(X_test_padded)\n",
        "    y_pred = y_pred_prob.argmax(axis=1)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Store results\n",
        "    results = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "O4CQL4rf5mhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Define URLs for dataset\n",
        "    url = \"urdu-sentiment-corpus-v1.tsv\"\n",
        "\n",
        "    # Test train_evaluate_bilstm_with_glove_embeddings function\n",
        "    print(\"Testing train_evaluate_bilstm_with_glove_embeddings function...\")\n",
        "    try:\n",
        "        rg_results = train_evaluate_bilstm_with_glove_embeddings(url)\n",
        "        print(\"Results with BiLSTM and GloVe embeddings:\")\n",
        "        print(\"Accuracy:\", rg_results['Accuracy'])\n",
        "        print(\"Precision:\", rg_results['Precision'])\n",
        "        print(\"Recall:\", rg_results['Recall'])\n",
        "        print(\"F1-Score:\", rg_results['F1-Score'])\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred while testing train_evaluate_bilstm_with_glove_embeddings:\", e)\n",
        "        print()\n",
        "\n",
        "    # Test train_evaluate_bilstm_with_fasttext_embeddings function\n",
        "    print(\"Testing train_evaluate_bilstm_with_fasttext_embeddings function...\")\n",
        "    try:\n",
        "        rw_results = train_evaluate_bilstm_with_fasttext_embeddings(url)\n",
        "        print(\"Results with BiLSTM and FastText embeddings:\")\n",
        "        print(\"Accuracy:\", rw_results['Accuracy'])\n",
        "        print(\"Precision:\", rw_results['Precision'])\n",
        "        print(\"Recall:\", rw_results['Recall'])\n",
        "        print(\"F1-Score:\", rw_results['F1-Score'])\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred while testing train_evaluate_bilstm_with_fasttext_embeddings:\", e)\n",
        "        print()\n",
        "\n",
        "    # Test train_evaluate_bilstm_without_pretrained_embeddings function\n",
        "    print(\"Testing train_evaluate_bilstm_without_pretrained_embeddings function...\")\n",
        "    try:\n",
        "        rn_results = train_evaluate_bilstm_without_pretrained_embeddings(url)\n",
        "        print(\"Results with BiLSTM without pre-trained embeddings:\")\n",
        "        print(\"Accuracy:\", rn_results['Accuracy'])\n",
        "        print(\"Precision:\", rn_results['Precision'])\n",
        "        print(\"Recall:\", rn_results['Recall'])\n",
        "        print(\"F1-Score:\", rn_results['F1-Score'])\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred while testing train_evaluate_bilstm_without_pretrained_embeddings:\", e)\n",
        "        print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qcXJVV4Z6WlC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}